{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Top 10 algorithms in data mining\n",
        "\n",
        "The top 10 algorithms are: \n",
        "\n",
        "1. C4.5\n",
        "2. k-Means\n",
        "3. SVM\n",
        "4. Apriori\n",
        "5. EM\n",
        "6. PageRank \n",
        "7. AdaBoost\n",
        "8. kNN\n",
        "9. Naive Bayes\n",
        "10.  CART. \n",
        "\n",
        "The 10 algorithms covered: \n",
        "* classification\n",
        "* clustering\n",
        "* statistical learning\n",
        "* association analysis\n",
        "*  link mining\n",
        "which are all among the most important topics in data mining research and development. \n",
        "\n",
        " \n",
        "The objective is simply to highlight what was considered by the community as the state-of-the-art data mining tools. "
      ],
      "metadata": {
        "id": "EuFoL0rFY2JV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1. k-means\n",
        "\n",
        "This is one of the workhorse unsupervised algorithms. \n",
        "* The goal of k-means is simply to cluster by proximity to a set of k points. \n",
        "  * By updating the locations of the `k` points according to the mean of the points closest to them, the algorithm iterates to the k-means. \n",
        "  \n",
        "[Example](https://github.com/Egade/notes/blob/main/070_Clustering.ipynb)"
      ],
      "metadata": {
        "id": "z7jELlW4ZxXi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. EM (mixture models)\n",
        "\n",
        "Mixture models are the another workhorse algorithm for unsupervised learning. \n",
        "* The assumption underlying the mixture models is that the observed data is produced by a mixture of different probability distribution functions whose weightings are unknown. \n",
        "  * Moreover, the parameters must be estimated, thus requiring the Expectation-Maximization (EM) algorithm. \n",
        "  \n",
        "Eg.\n"
      ],
      "metadata": {
        "id": "hI_z7_o2Z2Rb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Support vector machine (SVM)\n",
        "\n",
        "One of the most powerful and flexible supervised learning algorithms used for most of the 90s and 2000s, the SVM is an exceptional off-the-shelf method for classification and regression. \n",
        "\n",
        "The main idea: \n",
        "* project the data into higher dimensions and split the data with hyperplanes. \n",
        "\n",
        "Critical to making this work in practice was the kernel trick for efficiently evaluating inner products of functions in higher-dimensional space. \n",
        "\n",
        "Eg.\n"
      ],
      "metadata": {
        "id": "qtaPMOypZ7X1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. CART (Classification And Regression Tree)\n",
        "\n",
        "One of the most powerful technique of supervised learning. \n",
        "\n",
        "The main idea:\n",
        "* split the data in a principled and informed way so as to produce an interpretable clustering of the data. \n",
        "  * The data splitting occurs along a single variable at a time to produce branches of the tree structure. \n",
        "\n",
        "Eg.\n"
      ],
      "metadata": {
        "id": "ry_fWE-pZ_l9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. k-Nearest Neighbors (kNN)\n",
        "\n",
        "The simplest supervised algorithm to understand. \n",
        "It is highly interpretable and easy to execute. \n",
        "\n",
        "The main idea: \n",
        "* given a new data point $x_k$ which does not have a label, simply find the $k$ nearest neighbors $x_j$ with labels $y_j$. \n",
        "* The label of the new point $x_k$ is determined by a majority vote of the kNN. \n",
        "\n",
        "Eg."
      ],
      "metadata": {
        "id": "v73Vf9aaaDfO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. The Naive Bayes algorithm \n",
        "\n",
        "The Naive Bayes algorithm provides an intuitive framework for supervised learning. \n",
        "It is simple to construct and does not require any complicated parameter estimation, similar to SVM and/or classification trees. \n",
        "It further gives highly interpretable results that are remarkably good in practice. \n",
        "\n",
        "The main idea: \n",
        "\n",
        "* the method is based upon `Bayesâ€™s theorem` and the computation of conditional probabilities. \n",
        "  * one can estimate the label of a new data point based on the prior probability distributions of the labeled data.\n",
        "\n",
        "Eg."
      ],
      "metadata": {
        "id": "Jb60P3rLaIDc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. AdaBoost (ensemble learning and boosting)\n",
        "\n",
        "AdaBoost is an example of an ensemble learning algorithm. \n",
        "\n",
        "* AdaBoost is a form of random forest, which takes into account an ensemble of decision tree models. \n",
        "\n",
        "The way all boosting algorithms work is to first consider an equal weighting for all training data $x_j$. \n",
        "* Boosting re-weights the importance of the data according to how difficult they are to classify. \n",
        "* Thus the algorithm focuses on data that is harder to classify. \n",
        "  * Thus a family of weak learners can be trained to yield a strong learner by boosting the importance of hard to classify data. \n",
        "\n",
        "The concept and its usefulness are based upon a seminal theoretical contribution by Kearns and Valiant. \n",
        "* robust boosting & gradient boosting are the most powerful techniques.\n",
        "\n",
        "Eg.\n",
        "\n"
      ],
      "metadata": {
        "id": "kf8E0LTOaL0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. C4.5 (Ensemble learning of decision trees)\n",
        "\n",
        "This algorithm is another variant of decision tree learning developed by J. R. Quinlan. \n",
        "\n",
        "The main idea: \n",
        "\n",
        "* the algorithm splits the data according to an information entropy score. \n",
        "\n",
        "* it supports boosting as well as many other well known functionalities to improve performance. \n",
        "\n",
        "* broadly, we can think of this as a strong performing version of CART. \n",
        "\n",
        "Eg."
      ],
      "metadata": {
        "id": "_z9iv3E1aO9e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Apriori algorithm\n",
        "\n",
        "Use to find frequent itemsets from data. \n",
        "\n",
        "The main idea:\n",
        "\n",
        "  * Although, finding frequent itemsets from data may sound trivial, it is not since data sets tend to be very large and can easily produce NP-hard computations because of the combinatorial nature of the algorithms.\n",
        "* The Apriori algorithm provides an efficient algorithm for finding frequent itemsets using a candidate generation architecture. \n",
        "* This algorithm can then be used for fast learning of associate rules in the data.\n",
        "\n",
        "[Example](https://github.com/Egade/notes/blob/main/091_Apriori_algorithm.ipynb)"
      ],
      "metadata": {
        "id": "SbyyITRUaSgX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. PageRank\n",
        "\n",
        "The founding of Google by Sergey Brin and Larry Page revolved around the PageRank algorithm. \n",
        "\n",
        "* PageRank produces a static ranking of variables, such as web pages, by computing an off-line value for each variable that does not depend on search queries. \n",
        "\n",
        "* The PageRank is associated with graph theory as it originally interpreted a hyperlink from one page to another as a vote. \n",
        "  * From this, and various modifications of the original algorithm, one can then compute an importance score for each variable and provide an ordered rank list. T\n",
        "  * The number of enhancements for this algorithm is quite large. \n",
        "\n",
        "Eg."
      ],
      "metadata": {
        "id": "6BNLFuhzaU4c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!--NAVIGATION-->\n",
        "< [previous](prev) | [Contents](toc.ipynb) | [next](next.ipynb) >"
      ],
      "metadata": {
        "id": "mYLORyZKov_6"
      }
    }
  ]
}
